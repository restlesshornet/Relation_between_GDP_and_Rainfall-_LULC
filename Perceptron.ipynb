{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch \n",
    "from torch import nn \n",
    "from torchvision import transforms\n",
    "import pandas as pd \n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Final_Dataset.csv\")\n",
    "print(df)\n",
    "\n",
    "df.drop(['Time', 'Rainfall'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "columns_to_normalize = ['WasteLand', 'Evergreen Forest', 'Degraded/Scrub Fores', 'Plantation', \n",
    "                         'Kharif Crop', 'Zaid Crop', 'Decidous Forest', 'Waterbodies max', \n",
    "                         'Waterbodies min', 'Current Fallow', 'Double/Triple Crop', \n",
    "                         'Rabi Crop', 'Built-up', 'Grassland', 'Littoral Swamp','GDP']\n",
    "\n",
    "def Normalize(df, columns_to_normalize):\n",
    "# Extract the values from the DataFrame for normalization\n",
    "    data_to_normalize = df[columns_to_normalize].values\n",
    "\n",
    "    # Create a MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit and transform the data\n",
    "    normalized_data = scaler.fit_transform(data_to_normalize)\n",
    "\n",
    "    # Update the DataFrame with the normalized values\n",
    "    df[columns_to_normalize] = normalized_data\n",
    "\n",
    "    # Display the normalized DataFrame\n",
    "    print(df)\n",
    "    return df\n",
    "\n",
    "Normalize(df, columns_to_normalize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :16]\n",
    "Y = df.loc[:, ['GDP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def TabNet(X,Y):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.12)\n",
    "\n",
    "    kf = KFold(n_splits=4, random_state=42, shuffle=True)\n",
    "    predictions_array = []\n",
    "    CV_score_array = []\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_valid = X[train_index], X[test_index]\n",
    "        y_train, y_valid = Y[train_index], Y[test_index]\n",
    "\n",
    "        regressor = TabNetRegressor(verbose=0, seed=42, optimizer_fn=torch.optim.Adam)\n",
    "        regressor.fit(X_train=X_train, y_train=y_train,\n",
    "                    eval_set=[(X_valid, y_valid)],\n",
    "                    patience=300, max_epochs=2000,\n",
    "                    eval_metric=['rmse'],\n",
    "                    )\n",
    "        \n",
    "        CV_score_array.append(regressor.best_cost)\n",
    "        predictions_array.append(np.expm1(regressor.predict(np.array(X_test))))\n",
    "\n",
    "    predictions = np.mean(predictions_array, axis=0)\n",
    "    print(\"The CV score is %.5f\" % np.mean(CV_score_array,axis=0) )\n",
    "    return (\"The CV score is %.5f\" % np.mean(CV_score_array,axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TabNet(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor \n",
    "from sklearn.svm import SVR \n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error\n",
    "from sklearn.model_selection import RepeatedKFold, KFold, cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "cv_scores, cv_std = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(model):\n",
    "    return np.sqrt(-cross_val_score(model, X, Y, scoring=\"neg_mean_squared_error\", cv=kf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_learning_algorithm(model):\n",
    "    score = rmse(model)\n",
    "    cv_scores.append(score.mean())\n",
    "    cv_std.append(score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LGBMRegressor(objective='regression',\n",
    "                        num_leaves=166,\n",
    "                        learning_rate=0.05, \n",
    "                        n_estimators=120,\n",
    "                        max_bin = 55, \n",
    "                        bagging_fraction = 0.8,\n",
    "                        bagging_freq = 5, \n",
    "                        feature_fraction = 0.2319,\n",
    "                        feature_fraction_seed=9, \n",
    "                        bagging_seed=9,\n",
    "                        min_data_in_leaf =6, \n",
    "                        min_sum_hessian_in_leaf = 11),\n",
    "          SVR(kernel='rbf', C=10000, epsilon=0.05),\n",
    "          XGBRegressor(max_depth=7,learning_rate=0.05,\n",
    "                        n_estimators=700,\n",
    "                        min_child_weight=0.5, \n",
    "                        colsample_bytree=0.8, \n",
    "                        subsample=0.8, \n",
    "                        eta=0.5,\n",
    "                        seed=42)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['LGBMRegressor','SupportVectorRegressor','XGBRegressor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cv_scores, cv_std = [], []\n",
    "for model in models:\n",
    "    apply_learning_algorithm(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error \n",
    "\n",
    "print( \n",
    "  'mean_squared_error : ', mean_squared_error(Y, pred)) \n",
    "print( \n",
    "  'mean_absolute_error : ', mean_absolute_error(Y, pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.01)\n",
    "clf = MLPRegressor(random_state=1, max_iter=300).fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_squared_error :  2.9681331771499286e-31\n",
      "mean_absolute_error :  3.766828119263924e-16\n"
     ]
    }
   ],
   "source": [
    "models[0].fit(df.drop('GDP', axis=1), df['GDP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[0].fit(df.drop('GDP', axis=1), df['GDP'])\n",
    "models[2].fit(df.drop('GDP', axis=1), df['GDP'])\n",
    "\n",
    "# Number of synthetic data points to generate\n",
    "num_samples = 10\n",
    "\n",
    "# Generating synthetic data points\n",
    "synthetic_data_points = []\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    features = np.random.rand(df.shape[1] - 1)  # Random feature values excluding the target column\n",
    "    predictions = [model.predict([features])[0] for model in models]  # Use each model to predict\n",
    "    synthetic_data_points.append(predictions)\n",
    "\n",
    "# Convert synthetic data points to a DataFrame\n",
    "synthetic_data = pd.DataFrame(synthetic_data_points, columns=[f'Synthetic_{i}' for i in range(len(models))])\n",
    "\n",
    "# Convert s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
